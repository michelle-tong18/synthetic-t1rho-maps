{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603af99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all your packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ff102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deine split paths\n",
    "\"\"\"We can split the data in a number of different ways to test how the network performs\n",
    "when it sees different slices: excluding end slices, only slices with cartilage, middle slices, \n",
    "... . We can also test a subset of slices in each range so that training is faster.\"\"\"\n",
    "\n",
    "split_num = 28\n",
    "\n",
    "#code path\n",
    "split_path = '/data/knee_mri6/mwtong/imageSynthesis/splits'\n",
    "\n",
    "if split_num == 1:\n",
    "    save_path = split_path +'/001_all_Slices.csv'\n",
    "elif split_num == 2:\n",
    "    save_path = split_path +'/002_seg_Slices.csv'\n",
    "elif split_num == 3:\n",
    "    save_path = split_path +'/003_middle_Slice.csv'\n",
    "elif split_num == 4:\n",
    "    save_path = split_path +'/004_25to75Percent_Slices.csv'\n",
    "elif split_num == 5:\n",
    "    save_path = split_path +'/005_25to75percent_5RandomSlices.csv'\n",
    "elif split_num == 6:\n",
    "    save_path = split_path + '/006_25and75percent_Slice.csv'\n",
    "elif split_num == 7:\n",
    "    save_path = split_path +'/007_excludeOuter3_Slices.csv'\n",
    "elif split_num == 8:\n",
    "    save_path = split_path +'/008_excludeOuter3_4000TotalSlices.csv'\n",
    "elif split_num == 9:\n",
    "    save_path = split_path +'/009_excludeOuter3_8000TotalSlices.csv'\n",
    "elif split_num == 10:\n",
    "    save_path = split_path +'/010_excludeOuter3_4000TotalSlices.csv'\n",
    "elif split_num == 11:\n",
    "    save_path = split_path +'/011_25to75percent_EveryOtherSlice.csv'\n",
    "elif split_num == 12:\n",
    "    save_path = split_path +'/012_25and50and75percent_Slice.csv'\n",
    "elif split_num == 13:\n",
    "    save_path = split_path +'/013_25to75Percent_Slices.csv'\n",
    "elif split_num == 14:\n",
    "    save_path = split_path +'/014_25to75percent_Slices.csv'\n",
    "elif split_num == 15:\n",
    "    save_path = split_path +'/015_25to75percent_EveryOtherSlice.csv'\n",
    "elif split_num == 16:\n",
    "    save_path = split_path +'/016_middle_Slice.csv'\n",
    "elif split_num == 17:\n",
    "    save_path = split_path +'/017_25to75percent_EveryOtherSlice_ResearchOnly.csv'\n",
    "elif split_num == 18:\n",
    "    save_path = split_path +'/018_25to75Percent_Slices_ResearchOnly.csv'\n",
    "elif split_num == 19:\n",
    "    save_path = split_path +'/019_25to75Percent_EveryOtherSlice_ResearchOnly.csv'    \n",
    "elif split_num == 20:\n",
    "    save_path = split_path +'/020_25to75Percent_EveryOtherSlice_ResearchOnly.csv'  \n",
    "elif split_num == 21:\n",
    "    save_path = split_path +'/021_25to75Percent_EveryOtherSlice_NoKneeCoil.csv'  \n",
    "elif split_num == 22:\n",
    "    save_path = split_path + '/022_25to75Percent_Slices_NoKneeCoil.csv'\n",
    "elif split_num == 23:\n",
    "    save_path = split_path +'/023_25to75Percent_Slices.csv'\n",
    "elif split_num == 24:\n",
    "    save_path = split_path +'/024_25to75Percent_Slices.csv'\n",
    "elif split_num == 25:\n",
    "    save_path = split_path +'/025_25to75Percent_Slices_ClinicalOnly.csv'\n",
    "elif split_num == 26:\n",
    "    save_path = split_path +'/026_25to75Percent_Slices_ResearchOnly.csv'\n",
    "elif split_num == 27:\n",
    "    save_path = split_path +'/027_cartilage_Slices_ResearchOnly.csv'\n",
    "elif split_num == 28:\n",
    "    save_path = split_path +'/028_cartilage_Slices_KICKOnly.csv'\n",
    "    \n",
    "save_path_metadata = save_path.replace('.csv','_metadata.csv')\n",
    "\n",
    "# Load data\n",
    "if split_num <= 13:\n",
    "    raw_path = '/data/knee_mri6/mwtong/imageSynthesis/imgSynthesis_filepaths.csv'\n",
    "elif split_num < 19:\n",
    "    raw_path = '/data/knee_mri6/mwtong/imageSynthesis/imgSynthesis_filepaths_refit_t10.csv'\n",
    "elif split_num < 28:\n",
    "    raw_path = '/data/knee_mri6/mwtong/imageSynthesis/imgSynthesis_filepaths_refit_t100.csv'\n",
    "else:\n",
    "    raw_path = '/data/knee_mri6/mwtong/imageSynthesis/filepaths/imgSynthesis_filepaths_bilateral.csv'\n",
    "\n",
    "#Load the filepaths you extracted\n",
    "raw_df = pd.read_csv(raw_path)\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2434dc7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9537f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The input to the network needs each slice to have its own row in the dataframe. This \n",
    "cell calculates the total number of slices per patient. Then duplicates the patient row\n",
    "that many times. The slice number for each row is also added to the dataframe. \n",
    "Here the column idx is numbered 1-the total number of inputs to the network. The column session\n",
    "number corresponds to each case from the 3 studies we pulled data from. Recall some patients\n",
    "have multiple cases. \"\"\"\n",
    "\n",
    "#initialize variables\n",
    "num_repeats_list = []\n",
    "slice_num_list = []\n",
    "\n",
    "#determine how many times to copy each row in the dataframe\n",
    "for idx in range(0,len(raw_df)): #Select the row in the dataframe to work with\n",
    "    #total_slices = raw_df.loc[idx,'total slices e1']#Load path to the csv file with paths to echo1\n",
    "    mask_slices = raw_df.loc[idx,'cartilage mask slices']#Load path to the csv file with paths to echo1\n",
    "    mask_slices = mask_slices[1:-1].rsplit(',', 1)\n",
    "    \n",
    "    if split_num == 1:\n",
    "        slice_list = range(total_slices)\n",
    "        seed_num = 14\n",
    "    elif split_num == 2:\n",
    "        slice_list = range(int(mask_slices[0]),int(mask_slices[1])+1)\n",
    "        seed_num = 14\n",
    "    elif split_num == 3:\n",
    "        slice_list = [int(total_slices/2)]\n",
    "        seed_num = 14\n",
    "    elif split_num == 4:\n",
    "        slice_list = list(range(int(total_slices/4),int(total_slices*3/4)))\n",
    "        seed_num = 14\n",
    "    elif split_num == 5:\n",
    "        slice_list = random.sample(list(range(int(total_slices/4),int(total_slices*3/4))),5)\n",
    "        seed_num = 14\n",
    "    elif split_num == 6:\n",
    "        slice_list = [int(total_slices/4),int(total_slices*3/4)]\n",
    "        seed_num = 14\n",
    "    elif split_num == 7:\n",
    "        slice_list = list(range(3,total_slices-3))\n",
    "        seed_num = 14\n",
    "    elif split_num == 8:\n",
    "        slice_list = list(range(3,total_slices-3))\n",
    "        seed_num = 14\n",
    "        # total_cases set to 4000\n",
    "    elif split_num == 9:\n",
    "        slice_list = list(range(3,total_slices-3))\n",
    "        seed_num = 14\n",
    "        # total_cases set to 8000\n",
    "    elif split_num == 10:\n",
    "        slice_list = list(range(3,total_slices-3))\n",
    "        seed_num = 18\n",
    "        # total_cases set to 8000\n",
    "    elif split_num == 11:\n",
    "        slice_list = list(range(int(total_slices/4),int(total_slices*3/4),2))\n",
    "        seed_num = 18\n",
    "    elif split_num == 12:\n",
    "        slice_list = [int(total_slices/4),int(total_slices/2),int(total_slices*3/4)]\n",
    "        seed_num = 32\n",
    "    elif split_num == 13:\n",
    "        slice_list = list(range(int(total_slices/4),int(total_slices*3/4)))\n",
    "        seed_num = 32\n",
    "    elif split_num == 14:\n",
    "        slice_list = list(range(int(total_slices/4),int(total_slices*3/4)))\n",
    "        seed_num = 32\n",
    "    elif split_num == 15:\n",
    "        slice_list = list(range(int(total_slices/4),int(total_slices*3/4),2))\n",
    "        seed_num = 32\n",
    "    elif split_num == 16:\n",
    "        slice_list = [int(total_slices/2)]\n",
    "    elif split_num == 17:\n",
    "        slice_list = list(range(int(total_slices/4),int(total_slices*3/4),2))\n",
    "        seed_num = 68\n",
    "    elif split_num == 18:\n",
    "        slice_list = list(range(int(total_slices/4),int(total_slices*3/4)))\n",
    "        seed_num = 4\n",
    "    elif split_num == 19:\n",
    "        slice_list = list(range(int(total_slices/4),int(total_slices*3/4),2))\n",
    "        seed_num = 17\n",
    "    elif split_num == 20:\n",
    "        slice_list = list(range(int(total_slices/4),int(total_slices*3/4),2))\n",
    "        seed_num = 17\n",
    "    elif split_num == 21:\n",
    "        slice_list = list(range(int(total_slices/4),int(total_slices*3/4),2))\n",
    "        seed_num = 110\n",
    "    elif split_num == 22:\n",
    "        slice_list = list(range(int(total_slices/4),int(total_slices*3/4)))\n",
    "        seed_num = 32\n",
    "    elif split_num == 23:\n",
    "        slice_list = list(range(int(total_slices/4),int(total_slices*3/4)))\n",
    "        seed_num = 32\n",
    "    elif split_num == 24:\n",
    "        slice_list = list(range(int(total_slices/4),int(total_slices*3/4)))\n",
    "        seed_num = 17\n",
    "    elif split_num == 25:\n",
    "        slice_list = list(range(int(total_slices/4),int(total_slices*3/4)))\n",
    "        seed_num = 14\n",
    "    elif split_num == 26:\n",
    "        slice_list = list(range(int(total_slices/4),int(total_slices*3/4)))\n",
    "        seed_num = 4\n",
    "    elif split_num == 27:\n",
    "        slice_list = range(int(mask_slices[0]),int(mask_slices[1])+1)\n",
    "        seed_num = 22\n",
    "    elif split_num == 28:\n",
    "        slice_list = range(int(mask_slices[0]),int(mask_slices[1])+1)\n",
    "        seed_num = 22\n",
    "        \n",
    "    num_repeats_list.append(len(slice_list))\n",
    "    slice_num_list.extend(slice_list)\n",
    "\n",
    "df_analysis_slices = raw_df.loc[raw_df.index.repeat(num_repeats_list)].reset_index(drop=True)\n",
    "df_analysis_slices['slice number'] = slice_num_list\n",
    "df_analysis_slices['idx'] = df_analysis_slices.index\n",
    "df_analysis_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa2a148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a4e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b11e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove cases from the dataframe based on exclusion criteria \n",
    "\"\"\"Reasons for exclusion are included in the master imageSynthesis_filepaths.csv. Error \n",
    "incomplete info is determined when the age/sex/weight is read from the e1 dicoms. Error\n",
    "image artifacts are manually annotated in the csv file based on visual inspection of the\n",
    "QC images. Error exp fit is determine when the signal intensity similarity metrics are\n",
    "evalued after clippling.\n",
    "\n",
    "Note: above split number 13 - I started using intensity values clipped at 150 so the\n",
    "\"\"\"\n",
    "\n",
    "all_indices = list(range(len(df_analysis_slices)))\n",
    "\n",
    "# Used intensity values normalized to the 95th percentile in network. (2.5 percentile maps to 0,\n",
    "# and 97.5 percentile maps to 1)\n",
    "if split_num < 13:\n",
    "    remove_indices = np.concatenate((\n",
    "        np.where(df_analysis_slices['error incomplete info']==1)[0],\n",
    "        np.where(df_analysis_slices['error image artifacts']==1)[0],\n",
    "        np.where(df_analysis_slices['error exp fit']==1)[0] ))\n",
    "# Used intensity values clipped at 150 in network for later trainings.)\n",
    "elif split_num < 17:\n",
    "    remove_indices = np.concatenate((\n",
    "        np.where(df_analysis_slices['error incomplete info']==1)[0],\n",
    "        np.where(df_analysis_slices['error image artifacts']==1)[0],\n",
    "        np.where(df_analysis_slices['error exp fit 150 thresh']==1)[0] ))\n",
    "elif split_num < 21:\n",
    "    df_analysis_slices.loc[:,'research study'] = df_analysis_slices.loc[:,'research study'].replace({'vanitha2': 'vanitha'}, regex=True)\n",
    "    print(len(np.where(~(df_analysis_slices['research study']=='vanitha'))[0]))\n",
    "    remove_indices = np.concatenate((\n",
    "        np.where(df_analysis_slices['error incomplete info']==1)[0],\n",
    "        np.where(df_analysis_slices['error image artifacts']==1)[0],\n",
    "        np.where(df_analysis_slices['error exp fit 150 thresh']==1)[0],\n",
    "        np.where(df_analysis_slices['research study']=='vanitha')[0]))\n",
    "elif split_num < 23:\n",
    "    remove_indices = np.concatenate((\n",
    "        np.where(df_analysis_slices['error incomplete info']==1)[0],\n",
    "        np.where(df_analysis_slices['error image artifacts']==1)[0],\n",
    "        np.where(df_analysis_slices['error exp fit 150 thresh']==1)[0],\n",
    "        np.where(df_analysis_slices['receive coil']=='Knee')[0]))\n",
    "elif split_num < 25:\n",
    "    remove_indices = np.concatenate((\n",
    "        np.where(df_analysis_slices['error incomplete info']==1)[0],\n",
    "        np.where(df_analysis_slices['error image artifacts']==1)[0],\n",
    "        np.where(df_analysis_slices['error exp fit 150 thresh']==1)[0] ))\n",
    "elif split_num == 25:\n",
    "    remove_indices = np.concatenate((\n",
    "        np.where(df_analysis_slices['error incomplete info']==1)[0],\n",
    "        np.where(df_analysis_slices['error image artifacts']==1)[0],\n",
    "        np.where(df_analysis_slices['error exp fit 150 thresh']==1)[0],\n",
    "        np.where(df_analysis_slices['research study']=='P50_ACL')[0],\n",
    "        np.where(df_analysis_slices['research study']=='AF_UCSF')[0],\n",
    "        np.where(df_analysis_slices['research study']=='AF_HSS')[0],\n",
    "        np.where(df_analysis_slices['research study']=='AF_MAYO')[0] ))\n",
    "elif split_num == 26:\n",
    "    df_analysis_slices.loc[:,'research study'] = df_analysis_slices.loc[:,'research study'].replace({'vanitha2': 'vanitha'}, regex=True)\n",
    "    print(len(np.where(~(df_analysis_slices['research study']=='vanitha'))[0]))\n",
    "    remove_indices = np.concatenate((\n",
    "        np.where(df_analysis_slices['error incomplete info']==1)[0],\n",
    "        np.where(df_analysis_slices['error image artifacts']==1)[0],\n",
    "        np.where(df_analysis_slices['error exp fit 150 thresh']==1)[0],\n",
    "        np.where(df_analysis_slices['research study']=='vanitha')[0]))\n",
    "elif split_num == 27:\n",
    "    df_analysis_slices.loc[:,'research study'] = df_analysis_slices.loc[:,'research study'].replace({'vanitha2': 'vanitha'}, regex=True)\n",
    "    print(len(np.where(~(df_analysis_slices['research study']=='vanitha'))[0]))\n",
    "    remove_indices = np.concatenate((\n",
    "        np.where(df_analysis_slices['error incomplete info']==1)[0],\n",
    "        np.where(df_analysis_slices['error image artifacts']==1)[0],\n",
    "        np.where(df_analysis_slices['error exp fit 150 thresh']==1)[0],\n",
    "        np.where(df_analysis_slices['research study']=='vanitha')[0]))\n",
    "elif split_num == 28:\n",
    "    remove_indices = np.concatenate((\n",
    "        np.where(df_analysis_slices['error incomplete info']==1)[0],\n",
    "        np.where(df_analysis_slices['research study']!='KICK')[0]))\n",
    "\n",
    "\n",
    "print('total removed cases: ' + str(len(remove_indices)))\n",
    "print(remove_indices)\n",
    "analysis_indices = list(set(all_indices) - set(remove_indices))\n",
    "\n",
    "# Set the index column value to NaN using the dataframe row index\n",
    "df_remove_cases = df_analysis_slices.copy()\n",
    "df_remove_cases.loc[remove_indices, 'session number'] = np.nan\n",
    "\n",
    "# removing rows with nan or missing values\n",
    "df_remove_cases = df_remove_cases.dropna().reset_index(drop=True)\n",
    "df_remove_cases = df_remove_cases.astype({\"session number\": int})\n",
    "print('all data n=' + str(len(df_analysis_slices)))\n",
    "print('analysis data n=' + str(len(df_remove_cases)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755fb275",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd61ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the fractions of the available data that you want to be training, validation and test\n",
    "train_split_ratio = 0.65\n",
    "val_split_ratio   = 0.15\n",
    "test_split_ratio  = 0.20\n",
    "\n",
    "if split_num == 28:\n",
    "    train_split_ratio = 0\n",
    "    val_split_ratio   = 0\n",
    "    test_split_ratio  = 1\n",
    "    \n",
    "total_cases = len(df_remove_cases)\n",
    "if split_num == 8 or split_num == 10:\n",
    "    total_cases = 4000\n",
    "elif split_num == 9:\n",
    "    total_cases = 8000\n",
    "\n",
    "train_total_num_cases = int(train_split_ratio*total_cases)\n",
    "val_total_num_cases   = int(val_split_ratio*total_cases)\n",
    "test_total_num_cases  = int(test_split_ratio*total_cases)\n",
    "print(train_total_num_cases)\n",
    "print(val_total_num_cases)\n",
    "print(test_total_num_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a673e99-8f4f-4cb4-9a80-aab705cf8abf",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Identify the indices within your data that will become training, validation and test. Be sure to set a random seed\n",
    "so that your split is reproducible!\n",
    "\n",
    "For purposes of this example, I'm doing this split completely blind. But some things you'd have to consider: are there\n",
    "entries within your data you want to exclude from ending up in any of train, val or test (i.e. saturated maps)?\n",
    "Do you have a way to check demographics data on your splits (i.e. make sure that age, BMI, etc. distributions are\n",
    "similar across each of the 3 datasets)?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac5d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data such that each patient is either in the training, validation, OR testing set\n",
    "\"\"\"In order to ensure each patient is either in the train, val, or test set. I made a new\n",
    "dataframe with a column for each patient, their corresponding indicies in the dataframe\n",
    "with one row for each slice, and the total number of cases that can be input to the model.\"\"\"\n",
    "\n",
    "\n",
    "# Identify the number of scans and associated indices for each patient\n",
    "patient_num_scans = df_remove_cases.groupby('patient')['idx'].size()\n",
    "patient_scan_indicies = df_remove_cases.groupby('patient')['idx'].apply(list)\n",
    "\n",
    "frames = [patient_scan_indicies,patient_num_scans]\n",
    "df_patientInfo = pd.concat(frames,axis=1).reset_index()\n",
    "df_patientInfo.columns = ['patient','inds','total_scans']\n",
    "df_patientInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c6d864",
   "metadata": {},
   "source": [
    "# For one split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a8c558",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" FOR ONE SPLIT (NO CROSS VAL) \"\"\"\n",
    "\n",
    "# Randomly assign patients to the training/validation/testing set such that there \n",
    "# is the desired ratio of cases in each set\n",
    "\"\"\"Patients are randomly assigned to the training set as long as the number of cases for training\n",
    "is less than the total number of training cases we calculate earlier. This is repeated for the\n",
    "val and testing set. Any extra patients that were not assigned based on this criteria are \n",
    "assigned to the testing set. Recall, sometimes the total number of cases in the training and val \n",
    "set will be less. For example, the set may only need 1 more case, but all patients have 2+ cases \n",
    "since they all have multiple slices/scans.\"\"\"\n",
    "\n",
    "random.seed(seed_num)\n",
    "unassigned_patients = list(range(0,len(df_patientInfo)))\n",
    "\n",
    "#training data\n",
    "train_num_cases = 0;\n",
    "train_patient_inds = []\n",
    "while train_num_cases < train_total_num_cases and len(unassigned_patients)>0:\n",
    "    new_patient_idx = random.sample(unassigned_patients,1)[0]\n",
    "    #new_patient_info = df_patientInfo.iloc[new_patient_idx,:]\n",
    "    new_cases = df_patientInfo.loc[new_patient_idx,\"total_scans\"]\n",
    "\n",
    "    to_include = (train_num_cases+new_cases <= train_total_num_cases)\n",
    "    if to_include == True:\n",
    "        train_patient_inds.append(new_patient_idx)\n",
    "        train_num_cases = train_num_cases+new_cases\n",
    "    unassigned_patients.remove(new_patient_idx)\n",
    "train_patient_inds = np.sort(train_patient_inds).tolist()\n",
    "\n",
    "#validation data\n",
    "unassigned_patients = [i for i in range(0,len(df_patientInfo)) if i not in train_patient_inds]\n",
    "\n",
    "val_num_cases = 0;\n",
    "val_patient_inds = []\n",
    "while val_num_cases < val_total_num_cases and len(unassigned_patients)>0:\n",
    "    new_patient_idx = random.sample(unassigned_patients,1)[0]\n",
    "    #new_patient_info = df_patientInfo.iloc[new_patient_idx,:]\n",
    "    new_cases = df_patientInfo.loc[new_patient_idx,\"total_scans\"]\n",
    "    \n",
    "    to_include = (val_num_cases+new_cases <= val_total_num_cases)\n",
    "    if to_include == True:\n",
    "        val_patient_inds.append(new_patient_idx)\n",
    "        val_num_cases = val_num_cases+new_cases  \n",
    "    unassigned_patients.remove(new_patient_idx)\n",
    "val_patient_inds = np.sort(val_patient_inds).tolist()\n",
    "\n",
    "#testing data\n",
    "unassigned_patients = [i for i in range(0,len(df_patientInfo)) if i not in train_patient_inds]\n",
    "unassigned_patients = [i for i in unassigned_patients if i not in val_patient_inds]\n",
    "\n",
    "test_num_cases = 0;\n",
    "test_patient_inds = []\n",
    "while test_num_cases < test_total_num_cases and len(unassigned_patients)>0:\n",
    "    new_patient_idx = random.sample(unassigned_patients,1)[0]\n",
    "    #new_patient_info = df_patientInfo.iloc[new_patient_idx,:]\n",
    "    new_cases = df_patientInfo.loc[new_patient_idx,\"total_scans\"]\n",
    "\n",
    "    to_include = (test_num_cases+new_cases <= test_total_num_cases)\n",
    "    if to_include == True:\n",
    "        test_patient_inds.append(new_patient_idx)\n",
    "        test_num_cases = test_num_cases+new_cases\n",
    "    unassigned_patients.remove(new_patient_idx)\n",
    "test_patient_inds = np.sort(test_patient_inds).tolist()\n",
    "\n",
    "# Check whether all patients were assigned to a train/val/test set\n",
    "unassigned_patients = [i for i in range(0,len(df_patientInfo)) if i not in train_patient_inds]\n",
    "unassigned_patients = [i for i in unassigned_patients if i not in val_patient_inds]\n",
    "unassigned_patients = [i for i in unassigned_patients if i not in test_patient_inds]\n",
    "\n",
    "# Add patients that were not assigned to a set to the testing set\n",
    "check_split_num = True\n",
    "if (split_num == 8) or (split_num == 9) or (split_num == 10):\n",
    "    check_split_num = False\n",
    "\n",
    "if (len(df_patientInfo) > 0) and check_split_num:\n",
    "    print('extra unassigned patients: ', unassigned_patients)\n",
    "    print('\\tadding patients to testing set')\n",
    "    test_patient_inds.extend(unassigned_patients)\n",
    "\n",
    "# Check whether all patients were assigned to a set\n",
    "print('Do train/val/test patients add to the total number of patients?')\n",
    "check = (len(df_patientInfo) == (len(train_patient_inds)+len(val_patient_inds)+len(test_patient_inds)))\n",
    "print('\\t',check,',',len(train_patient_inds)+len(val_patient_inds)+len(test_patient_inds),'=',len(df_patientInfo))\n",
    "#print(train_patient_inds)\n",
    "#print(val_patient_inds)\n",
    "#print(test_patient_inds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b454a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f47d6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af89f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices for all patient SCANS in train/val/test datasets\n",
    "\"\"\"Having sorting the patients into train/val/test sets, we can use the inds column of the\n",
    "patient dataframe to identify the train/val/test rows in the dataframe with one row for each \n",
    "slice of interest \"\"\"\n",
    "\n",
    "# find train indices from train patients\n",
    "train_data_inds = []\n",
    "for new_patient_idx in train_patient_inds:\n",
    "    new_patient_inds = df_patientInfo.loc[new_patient_idx,\"inds\"]\n",
    "    train_data_inds.extend(new_patient_inds) \n",
    "print('Is the number of train cases consistent with the split ratio:',train_split_ratio,'?')\n",
    "check = (len(train_data_inds) == train_total_num_cases)\n",
    "print('\\t',check,',',len(train_data_inds),'=',train_total_num_cases)\n",
    "\n",
    "# find validation indices from val patients\n",
    "val_data_inds = []\n",
    "for new_patient_idx in val_patient_inds:\n",
    "    new_patient_inds = df_patientInfo.loc[new_patient_idx,\"inds\"]\n",
    "    val_data_inds.extend(new_patient_inds) \n",
    "print('Is the number of validation cases consistent with the split ratio:',val_split_ratio,'?')\n",
    "check = (len(val_data_inds) == val_total_num_cases)\n",
    "print('\\t',check,',',len(val_data_inds),'=',val_total_num_cases)\n",
    "\n",
    "# find test indices from test patients\n",
    "test_data_inds = []\n",
    "for new_patient_idx in test_patient_inds:\n",
    "    new_patient_inds = df_patientInfo.loc[new_patient_idx,\"inds\"]\n",
    "    test_data_inds.extend(new_patient_inds) \n",
    "print('Is the number of test cases consistent with the split ratio:',test_split_ratio,'?')\n",
    "check = (len(test_data_inds) >= test_total_num_cases)\n",
    "print('\\t',check,',',len(test_data_inds),'>=',test_total_num_cases)\n",
    "\n",
    "# Check whether all scan data were assigned to a set\n",
    "print('Do train/val/test cases add to the total number of cases?')\n",
    "check = (len(df_remove_cases) == (len(train_data_inds)+len(val_data_inds)+len(test_data_inds)))\n",
    "print('\\t',check,',',len(train_data_inds)+len(val_data_inds)+len(test_data_inds),'=',len(df_remove_cases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6be113",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make a new column in the dataframe called 'set' and assign appropriate indices to it. Here, we designate 0 as being\n",
    "for train, 1 as for val, and 2 as for test. We'll exploit this in later code\n",
    "\"\"\"\n",
    "\n",
    "df_analysis_slices['set'] = -1\n",
    "df_analysis_slices.loc[train_data_inds,'set'] = 0\n",
    "df_analysis_slices.loc[val_data_inds,'set']   = 1\n",
    "df_analysis_slices.loc[test_data_inds,'set']  = 2\n",
    "\n",
    "#Just as a sanity check, make sure every index was assigned to one of the three datasets in the ratios you wanted\n",
    "df_analysis_slices['set'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e03d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_analysis_slices['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check patient information for the train/val/test sets\n",
    "\"\"\"Check the distribution of age, weight, sex, and study between the train/val/test sets.\n",
    "If they are not similar, you can change the random seed in one of the cells at the top and\n",
    "see if the split is more even. Note, thus far Valentia, Aniket, and I have decided to group\n",
    "the studies into P50, AF, and Vanithas. I initially noticed AF is a multi-institution study \n",
    "from UCSF, MAYO, and HSS. Also, the filepath to vanithas included vanithas and vanithas2. \n",
    "Since we are not making these distinctions, we do need to regroup AF and vanithas.\n",
    "\"\"\"\n",
    "\n",
    "#age\n",
    "train_ages = [int(i[:-1]) for i in df_analysis_slices.loc[train_data_inds,'age']]\n",
    "val_ages = [int(i[:-1]) for i in df_analysis_slices.loc[val_data_inds,'age']]\n",
    "test_ages = [int(i[:-1]) for i in df_analysis_slices.loc[test_data_inds,'age']]\n",
    "\n",
    "\n",
    "age_mean_train_val_test = [np.mean(train_ages),np.mean(val_ages),np.mean(test_ages)];\n",
    "age_std_train_val_test = [np.std(train_ages),np.std(val_ages),np.std(test_ages)];\n",
    "age_median_train_val_test = [np.median(train_ages),np.median(val_ages),np.median(test_ages)];\n",
    "\n",
    "weight_mean_train_val_test = [df_analysis_slices.loc[train_data_inds,'weight'].mean(),\n",
    "                              df_analysis_slices.loc[val_data_inds,'weight'].mean(),\n",
    "                              df_analysis_slices.loc[test_data_inds,'weight'].mean()];\n",
    "weight_std_train_val_test = [df_analysis_slices.loc[train_data_inds,'weight'].std(),\n",
    "                              df_analysis_slices.loc[val_data_inds,'weight'].std(),\n",
    "                              df_analysis_slices.loc[test_data_inds,'weight'].std()];\n",
    "weight_median_train_val_test = [df_analysis_slices.loc[train_data_inds,'weight'].median(),\n",
    "                              df_analysis_slices.loc[val_data_inds,'weight'].median(),\n",
    "                              df_analysis_slices.loc[test_data_inds,'weight'].median()];\n",
    "\n",
    "total_cases_train_val_test = [len(df_analysis_slices.loc[train_data_inds]),\n",
    "                              len(df_analysis_slices.loc[val_data_inds]),\n",
    "                              len(df_analysis_slices.loc[test_data_inds])]\n",
    "\n",
    "sex_F_train_val_test = [df_analysis_slices.loc[train_data_inds,'sex'].to_list().count('F'),\n",
    "                       df_analysis_slices.loc[val_data_inds,'sex'].to_list().count('F'),\n",
    "                       df_analysis_slices.loc[test_data_inds,'sex'].to_list().count('F')]\n",
    "sex_M_train_val_test = [df_analysis_slices.loc[train_data_inds,'sex'].to_list().count('M'),\n",
    "                       df_analysis_slices.loc[val_data_inds,'sex'].to_list().count('M'),\n",
    "                       df_analysis_slices.loc[test_data_inds,'sex'].to_list().count('M')]\n",
    "try:\n",
    "    sex_ratio_MtoF_train_val_test = [(sex_M_train_val_test[i])/(sex_F_train_val_test[i]) for i in range(0,len(sex_F_train_val_test))]\n",
    "except:\n",
    "    sex_ratio_MtoF_train_val_test = []\n",
    "    for i in range(0,len(sex_F_train_val_test)):\n",
    "        if sex_F_train_val_test[i]>0:\n",
    "            sex_ratio_MtoF_train_val_test.append((sex_M_train_val_test[i])/(sex_F_train_val_test[i]))\n",
    "        else: \n",
    "            sex_ratio_MtoF_train_val_test.append(0)\n",
    "        \n",
    "p50_train_val_test = [df_analysis_slices.loc[train_data_inds,'research study'].to_list().count('P50_ACL'),\n",
    "                       df_analysis_slices.loc[val_data_inds,'research study'].to_list().count('P50_ACL'),\n",
    "                       df_analysis_slices.loc[test_data_inds,'research study'].to_list().count('P50_ACL')]\n",
    "AFUCSF_train_val_test = [df_analysis_slices.loc[train_data_inds,'research study'].to_list().count('AF_UCSF'),\n",
    "                        df_analysis_slices.loc[val_data_inds,'research study'].to_list().count('AF_UCSF'),\n",
    "                        df_analysis_slices.loc[test_data_inds,'research study'].to_list().count('AF_UCSF')]\n",
    "AFMAYO_train_val_test = [df_analysis_slices.loc[train_data_inds,'research study'].to_list().count('AF_MAYO'),\n",
    "                        df_analysis_slices.loc[val_data_inds,'research study'].to_list().count('AF_MAYO'),\n",
    "                        df_analysis_slices.loc[test_data_inds,'research study'].to_list().count('AF_MAYO')]\n",
    "AFHSS_train_val_test = [df_analysis_slices.loc[train_data_inds,'research study'].to_list().count('AF_HSS'),\n",
    "                        df_analysis_slices.loc[val_data_inds,'research study'].to_list().count('AF_HSS'),\n",
    "                        df_analysis_slices.loc[test_data_inds,'research study'].to_list().count('AF_HSS')]\n",
    "vanita_train_val_test = [sum(x.startswith('vanitha') for x in df_analysis_slices.loc[train_data_inds,'research study'].to_list()),\n",
    "                       sum(x.startswith('vanitha') for x in df_analysis_slices.loc[val_data_inds,'research study'].to_list()),\n",
    "                       sum(x.startswith('vanitha') for x in df_analysis_slices.loc[test_data_inds,'research study'].to_list())]\n",
    "'''\n",
    "p50_ratio = [p50_train_val_test[i]/total_cases_train_val_test[i] for i in range(len(total_cases_train_val_test))]\n",
    "AFUCSF_ratio = [AFUCSF_train_val_test[i]/total_cases_train_val_test[i] for i in range(len(total_cases_train_val_test))]\n",
    "AFMAYO_ratio = [AFMAYO_train_val_test[i]/total_cases_train_val_test[i] for i in range(len(total_cases_train_val_test))]\n",
    "AFHSS_ratio = [AFHSS_train_val_test[i]/total_cases_train_val_test[i] for i in range(len(total_cases_train_val_test))]\n",
    "vanita_ratio = [vanita_train_val_test[i]/total_cases_train_val_test[i] for i in range(len(total_cases_train_val_test))]\n",
    "\n",
    "timepoint_1 = [df_analysis_slices.loc[train_data_inds,'timepoint'].to_list().count(1),\n",
    "                       df_analysis_slices.loc[val_data_inds,'timepoint'].to_list().count(1),\n",
    "                       df_analysis_slices.loc[test_data_inds,'timepoint'].to_list().count(1)]\n",
    "timepoint_2 = [df_analysis_slices.loc[train_data_inds,'timepoint'].to_list().count(2),\n",
    "                       df_analysis_slices.loc[val_data_inds,'timepoint'].to_list().count(2),\n",
    "                       df_analysis_slices.loc[test_data_inds,'timepoint'].to_list().count(2)]\n",
    "timepoint_3 = [df_analysis_slices.loc[train_data_inds,'timepoint'].to_list().count(3),\n",
    "                       df_analysis_slices.loc[val_data_inds,'timepoint'].to_list().count(3),\n",
    "                       df_analysis_slices.loc[test_data_inds,'timepoint'].to_list().count(3)]\n",
    "timepoint_4up = [len(df_analysis_slices.loc[train_data_inds])-(timepoint_1[0]+timepoint_2[0]+timepoint_3[0]),\n",
    "                len(df_analysis_slices.loc[val_data_inds])-(timepoint_1[1]+timepoint_2[1]+timepoint_3[1]),\n",
    "                len(df_analysis_slices.loc[test_data_inds])-(timepoint_1[2]+timepoint_2[2]+timepoint_3[2])]\n",
    "train_timepoints = np.round(np.array([timepoint_1[0], timepoint_2[0], timepoint_3[0], timepoint_4up[0]])/len(df_analysis_slices.loc[train_data_inds]),2)\n",
    "val_timepoints = np.round(np.array([timepoint_1[1], timepoint_2[1], timepoint_3[1], timepoint_4up[1]])/len(df_analysis_slices.loc[val_data_inds]),2)\n",
    "test_timepoints = np.round(np.array([timepoint_1[2], timepoint_2[2], timepoint_3[2], timepoint_4up[2]])/len(df_analysis_slices.loc[test_data_inds]),2)\n",
    "timepoints_train_val_test=[str(train_timepoints[0])+':'+str(train_timepoints[1])+':'+str(train_timepoints[2])+':'+str(train_timepoints[3]),\n",
    "                           str(val_timepoints[0])+':'+str(val_timepoints[1])+':'+str(val_timepoints[2])+':'+str(val_timepoints[3]),\n",
    "                          str(test_timepoints[0])+':'+str(test_timepoints[1])+':'+str(test_timepoints[2])+':'+str(test_timepoints[3])]\n",
    "'''\n",
    "data = {'set': ['train','val','test'],\n",
    "        'age avg':age_mean_train_val_test, \n",
    "        'age stdev':age_std_train_val_test, \n",
    "        'age median':age_median_train_val_test,\n",
    "        'weight avg':weight_mean_train_val_test, \n",
    "        'weight std':weight_std_train_val_test, \n",
    "        'weight median':weight_median_train_val_test,\n",
    "        '# cases':total_cases_train_val_test,\n",
    "        '# females':sex_F_train_val_test, \n",
    "        '# males':sex_M_train_val_test, \n",
    "        'M:F':sex_ratio_MtoF_train_val_test,\n",
    "#        'P50 % in set':p50_ratio,\n",
    "#        'AF_study % in set':str([AFUCSF_ratio,AFMAYO_ratio,AFHSS_ratio]),\n",
    "#        'vanitha % in set':vanita_ratio,\n",
    "#        'timepoint ratio 1:2:3:4+':timepoints_train_val_test\n",
    "        }\n",
    "\n",
    "df_metadata = pd.DataFrame(data)\n",
    "df_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8929efee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac1c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save your dataframe\n",
    "df_analysis_slices.to_csv(save_path,index = False)\n",
    "df_metadata.to_csv(save_path_metadata,index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b5ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cb460c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fc8a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a8112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a random split\n",
    "#random.seed(14)\n",
    "#train_inds  = random.sample(analysis_indices,int(len(raw_df)*train_split_ratio))\n",
    "\n",
    "#remaining_indices = [i for i in analysis_indices if i not in train_inds]\n",
    "\n",
    "#val_inds    = random.sample(remaining_indices,int(len(raw_df)*val_split_ratio))\n",
    "\n",
    "#test_inds   = [i for i in remaining_indices if i not in val_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0045ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab6ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce917827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb309c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RUN THIS CELL FOR CROSS VALIDATION \"\"\"\n",
    "\n",
    "# Randomly assign patients to the training/validation/testing set such that there \n",
    "# is the desired ratio of cases in each set\n",
    "\"\"\"Patients are randomly assigned to the training set as long as the number of cases for training\n",
    "is less than the total number of training cases we calculate earlier. This is repeated for the\n",
    "val and testing set. Any extra patients that were not assigned based on this criteria are \n",
    "assigned to the testing set. Recall, sometimes the total number of cases in the training and val \n",
    "set will be less. For example, the set may only need 1 more case, but all patients have 2+ cases \n",
    "since they all have multiple slices/scans.\"\"\"\n",
    "\n",
    "random.seed(seed_num)\n",
    "unassigned_patients = list(range(0,len(df_patientInfo)))\n",
    "\n",
    "#training data\n",
    "k_folds = 5\n",
    "fold_total_num_cases = round((train_total_num_cases+val_total_num_cases)/k_folds)\n",
    "\n",
    "fold1_patient_inds = []\n",
    "fold2_patient_inds = []\n",
    "fold3_patient_inds = []\n",
    "fold4_patient_inds = []\n",
    "fold5_patient_inds = []\n",
    "for k in np.arange(1,6):\n",
    "    train_patient_inds = []\n",
    "    train_num_cases = 0;\n",
    "    \n",
    "    unassigned_patients = [i for i in range(0,len(df_patientInfo)) if i not in fold1_patient_inds]\n",
    "    unassigned_patients = [i for i in unassigned_patients if i not in fold2_patient_inds]\n",
    "    unassigned_patients = [i for i in unassigned_patients if i not in fold3_patient_inds]\n",
    "    unassigned_patients = [i for i in unassigned_patients if i not in fold4_patient_inds]\n",
    "    unassigned_patients = [i for i in unassigned_patients if i not in fold5_patient_inds]\n",
    "    \n",
    "    print(len(unassigned_patients)>0)\n",
    "    while train_num_cases < fold_total_num_cases and len(unassigned_patients)>0:\n",
    "        new_patient_idx = random.sample(unassigned_patients,1)[0]\n",
    "        #new_patient_info = df_patientInfo.iloc[new_patient_idx,:]\n",
    "        new_cases = df_patientInfo.loc[new_patient_idx,\"total_scans\"]\n",
    "\n",
    "        to_include = (train_num_cases+new_cases <= fold_total_num_cases)\n",
    "        if to_include == True:\n",
    "            train_patient_inds.append(new_patient_idx)\n",
    "            train_num_cases = train_num_cases+new_cases\n",
    "        unassigned_patients.remove(new_patient_idx)\n",
    "    train_patient_inds = np.sort(train_patient_inds).tolist()\n",
    "    \n",
    "    if k==1:\n",
    "        fold1_patient_inds = np.copy(train_patient_inds)\n",
    "    elif k==2:\n",
    "        fold2_patient_inds = np.copy(train_patient_inds)\n",
    "    elif k==3:\n",
    "        fold3_patient_inds = np.copy(train_patient_inds)\n",
    "    elif k==4:\n",
    "        fold4_patient_inds = np.copy(train_patient_inds)\n",
    "    elif k==5:\n",
    "        fold5_patient_inds = np.copy(train_patient_inds)\n",
    "\n",
    "#testing data\n",
    "unassigned_patients = [i for i in range(0,len(df_patientInfo)) if i not in fold1_patient_inds]\n",
    "unassigned_patients = [i for i in unassigned_patients if i not in fold2_patient_inds]\n",
    "unassigned_patients = [i for i in unassigned_patients if i not in fold3_patient_inds]\n",
    "unassigned_patients = [i for i in unassigned_patients if i not in fold4_patient_inds]\n",
    "unassigned_patients = [i for i in unassigned_patients if i not in fold5_patient_inds]\n",
    "\n",
    "test_num_cases = 0;\n",
    "test_patient_inds = []\n",
    "while test_num_cases < test_total_num_cases and len(unassigned_patients)>0:\n",
    "    new_patient_idx = random.sample(unassigned_patients,1)[0]\n",
    "    #new_patient_info = df_patientInfo.iloc[new_patient_idx,:]\n",
    "    new_cases = df_patientInfo.loc[new_patient_idx,\"total_scans\"]\n",
    "\n",
    "    to_include = (test_num_cases+new_cases <= test_total_num_cases)\n",
    "    if to_include == True:\n",
    "        test_patient_inds.append(new_patient_idx)\n",
    "        test_num_cases = test_num_cases+new_cases\n",
    "    unassigned_patients.remove(new_patient_idx)\n",
    "test_patient_inds = np.sort(test_patient_inds).tolist()\n",
    "\n",
    "# Check whether all patients were assigned to a train/val/test set\n",
    "unassigned_patients = [i for i in range(0,len(df_patientInfo)) if i not in fold1_patient_inds]\n",
    "unassigned_patients = [i for i in unassigned_patients if i not in fold2_patient_inds]\n",
    "unassigned_patients = [i for i in unassigned_patients if i not in fold3_patient_inds]\n",
    "unassigned_patients = [i for i in unassigned_patients if i not in fold4_patient_inds]\n",
    "unassigned_patients = [i for i in unassigned_patients if i not in fold5_patient_inds]\n",
    "unassigned_patients = [i for i in unassigned_patients if i not in test_patient_inds]\n",
    "\n",
    "# Add patients that were not assigned to a set to the testing set\n",
    "check_split_num = True\n",
    "if (split_num == 8) or (split_num == 9) or (split_num == 10):\n",
    "    check_split_num = False\n",
    "\n",
    "if (len(df_patientInfo) > 0) and check_split_num:\n",
    "    print('extra unassigned patients: ', unassigned_patients)\n",
    "    print('\\tadding patients to testing set')\n",
    "    test_patient_inds.extend(unassigned_patients)\n",
    "\n",
    "# Check whether all patients were assigned to a set\n",
    "print('Do train/val/test patients add to the total number of patients?')\n",
    "check = (len(df_patientInfo) == (len(fold1_patient_inds)+len(fold2_patient_inds)+len(fold3_patient_inds)+\n",
    "      len(fold4_patient_inds)+len(fold5_patient_inds)+len(test_patient_inds)))\n",
    "print('\\t',check,',',len(fold1_patient_inds)+len(fold2_patient_inds)+len(fold3_patient_inds)+\n",
    "      len(fold4_patient_inds)+len(fold5_patient_inds)+len(test_patient_inds),'=',len(df_patientInfo))\n",
    "#print(train_patient_inds)\n",
    "#print(val_patient_inds)\n",
    "#print(test_patient_inds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd0f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices for all patient SCANS in train/val/test datasets\n",
    "\"\"\"Having sorting the patients into train/val/test sets, we can use the inds column of the\n",
    "patient dataframe to identify the train/val/test rows in the dataframe with one row for each \n",
    "slice of interest \"\"\"\n",
    "\n",
    "# find train indices from train patients\n",
    "fold1_data_inds = []\n",
    "fold2_data_inds = []\n",
    "fold3_data_inds = []\n",
    "fold4_data_inds = []\n",
    "fold5_data_inds = []\n",
    "for k in np.arange(1,6):\n",
    "    if k==1:\n",
    "        train_patient_inds = fold1_patient_inds\n",
    "    elif k==2:\n",
    "        train_patient_inds = fold2_patient_inds\n",
    "    elif k==3:\n",
    "        train_patient_inds = fold3_patient_inds\n",
    "    elif k==4:\n",
    "        train_patient_inds = fold4_patient_inds\n",
    "    elif k==5:\n",
    "        train_patient_inds = fold5_patient_inds\n",
    "    \n",
    "    train_data_inds = []\n",
    "    for new_patient_idx in train_patient_inds:\n",
    "        new_patient_inds = df_patientInfo.loc[new_patient_idx,\"inds\"]\n",
    "        train_data_inds.extend(new_patient_inds) \n",
    "    \n",
    "    if k==1:\n",
    "        fold1_data_inds = train_data_inds\n",
    "    elif k==2:\n",
    "        fold2_data_inds = train_data_inds\n",
    "    elif k==3:\n",
    "        fold3_data_inds = train_data_inds\n",
    "    elif k==4:\n",
    "        fold4_data_inds = train_data_inds\n",
    "    elif k==5:\n",
    "        fold5_data_inds = train_data_inds\n",
    "    \n",
    "    print('Is the number of train cases consistent with the split ratio:',train_split_ratio,'?')\n",
    "    check = (len(train_data_inds) == fold_total_num_cases)\n",
    "    print('\\t',check,',',len(train_data_inds),'=',fold_total_num_cases)\n",
    "    \n",
    "\n",
    "# find test indices from test patients\n",
    "test_data_inds = []\n",
    "for new_patient_idx in test_patient_inds:\n",
    "    new_patient_inds = df_patientInfo.loc[new_patient_idx,\"inds\"]\n",
    "    test_data_inds.extend(new_patient_inds) \n",
    "print('Is the number of test cases consistent with the split ratio:',test_split_ratio,'?')\n",
    "check = (len(test_data_inds) >= test_total_num_cases)\n",
    "print('\\t',check,',',len(test_data_inds),'>=',test_total_num_cases)\n",
    "\n",
    "# Check whether all scan data were assigned to a set\n",
    "print('Do train/val/test cases add to the total number of cases?')\n",
    "check = (len(df_remove_cases) == (len(fold1_data_inds)+len(fold2_data_inds)+len(fold3_data_inds)\n",
    "                                  +len(fold4_data_inds)+len(fold5_data_inds)+len(test_data_inds)))\n",
    "print('\\t',check,',',len(fold1_data_inds)+len(fold2_data_inds)+len(fold3_data_inds)+len(fold4_data_inds)+\n",
    "                                  len(fold5_data_inds)+len(test_data_inds),'=',len(df_remove_cases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8b1d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac3dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata_folds=pd.DataFrame()\n",
    "all_train_inds = np.concatenate([fold1_data_inds,fold2_data_inds,fold3_data_inds,fold4_data_inds,fold5_data_inds])\n",
    "for k in np.arange(1,6):\n",
    "    if k==1:\n",
    "         val_data_inds= fold1_data_inds\n",
    "    elif k==2:\n",
    "         val_data_inds= fold2_data_inds\n",
    "    elif k==3:\n",
    "         val_data_inds= fold3_data_inds\n",
    "    elif k==4:\n",
    "         val_data_inds= fold4_data_inds\n",
    "    elif k==5:\n",
    "         val_data_inds= fold5_data_inds\n",
    "     \n",
    "    train_data_inds = [i for i in all_train_inds if i not in val_data_inds]\n",
    "    \n",
    "    \"\"\"\n",
    "    Make a new column in the dataframe called 'set' and assign appropriate indices to it. Here, we designate 0 as being\n",
    "    for train, 1 as for val, and 2 as for test. We'll exploit this in later code\n",
    "    \"\"\"\n",
    "\n",
    "    df_analysis_slices['set'] = -1\n",
    "    df_analysis_slices.loc[train_data_inds,'set'] = 0\n",
    "    df_analysis_slices.loc[val_data_inds,'set']   = 1\n",
    "    df_analysis_slices.loc[test_data_inds,'set']  = 2\n",
    "\n",
    "    #Just as a sanity check, make sure every index was assigned to one of the three datasets in the ratios you wanted\n",
    "    df_analysis_slices['set'].value_counts()\n",
    "    \n",
    "    #Check patient information for the train/val/test sets\n",
    "    \"\"\"Check the distribution of age, weight, sex, and study between the train/val/test sets.\n",
    "    If they are not similar, you can change the random seed in one of the cells at the top and\n",
    "    see if the split is more even. Note, thus far Valentia, Aniket, and I have decided to group\n",
    "    the studies into P50, AF, and Vanithas. I initially noticed AF is a multi-institution study \n",
    "    from UCSF, MAYO, and HSS. Also, the filepath to vanithas included vanithas and vanithas2. \n",
    "    Since we are not making these distinctions, we do need to regroup AF and vanithas.\n",
    "    \"\"\"\n",
    "\n",
    "    #age\n",
    "    train_ages = [int(i[:-1]) for i in df_analysis_slices.loc[train_data_inds,'age']]\n",
    "    val_ages = [int(i[:-1]) for i in df_analysis_slices.loc[val_data_inds,'age']]\n",
    "    test_ages = [int(i[:-1]) for i in df_analysis_slices.loc[test_data_inds,'age']]\n",
    "\n",
    "\n",
    "    age_mean_train_val_test = [np.mean(train_ages),np.mean(val_ages),np.mean(test_ages)];\n",
    "    age_std_train_val_test = [np.std(train_ages),np.std(val_ages),np.std(test_ages)];\n",
    "    age_median_train_val_test = [np.median(train_ages),np.median(val_ages),np.median(test_ages)];\n",
    "\n",
    "    weight_mean_train_val_test = [df_analysis_slices.loc[train_data_inds,'weight'].mean(),\n",
    "                                  df_analysis_slices.loc[val_data_inds,'weight'].mean(),\n",
    "                                  df_analysis_slices.loc[test_data_inds,'weight'].mean()];\n",
    "    weight_std_train_val_test = [df_analysis_slices.loc[train_data_inds,'weight'].std(),\n",
    "                                  df_analysis_slices.loc[val_data_inds,'weight'].std(),\n",
    "                                  df_analysis_slices.loc[test_data_inds,'weight'].std()];\n",
    "    weight_median_train_val_test = [df_analysis_slices.loc[train_data_inds,'weight'].median(),\n",
    "                                  df_analysis_slices.loc[val_data_inds,'weight'].median(),\n",
    "                                  df_analysis_slices.loc[test_data_inds,'weight'].median()];\n",
    "\n",
    "    total_cases_train_val_test = [len(df_analysis_slices.loc[train_data_inds]),\n",
    "                                  len(df_analysis_slices.loc[val_data_inds]),\n",
    "                                  len(df_analysis_slices.loc[test_data_inds])]\n",
    "\n",
    "    sex_F_train_val_test = [df_analysis_slices.loc[train_data_inds,'sex'].to_list().count('F'),\n",
    "                           df_analysis_slices.loc[val_data_inds,'sex'].to_list().count('F'),\n",
    "                           df_analysis_slices.loc[test_data_inds,'sex'].to_list().count('F')]\n",
    "    sex_M_train_val_test = [df_analysis_slices.loc[train_data_inds,'sex'].to_list().count('M'),\n",
    "                           df_analysis_slices.loc[val_data_inds,'sex'].to_list().count('M'),\n",
    "                           df_analysis_slices.loc[test_data_inds,'sex'].to_list().count('M')]\n",
    "    sex_ratio_MtoF_train_val_test = [(sex_M_train_val_test[i])/(sex_F_train_val_test[i]) for i in range(0,len(sex_F_train_val_test))]\n",
    "\n",
    "\n",
    "    p50_train_val_test = [df_analysis_slices.loc[train_data_inds,'research study'].to_list().count('P50_ACL'),\n",
    "                           df_analysis_slices.loc[val_data_inds,'research study'].to_list().count('P50_ACL'),\n",
    "                           df_analysis_slices.loc[test_data_inds,'research study'].to_list().count('P50_ACL')]\n",
    "    AFUCSF_train_val_test = [df_analysis_slices.loc[train_data_inds,'research study'].to_list().count('AF_UCSF'),\n",
    "                            df_analysis_slices.loc[val_data_inds,'research study'].to_list().count('AF_UCSF'),\n",
    "                            df_analysis_slices.loc[test_data_inds,'research study'].to_list().count('AF_UCSF')]\n",
    "    AFMAYO_train_val_test = [df_analysis_slices.loc[train_data_inds,'research study'].to_list().count('AF_MAYO'),\n",
    "                            df_analysis_slices.loc[val_data_inds,'research study'].to_list().count('AF_MAYO'),\n",
    "                            df_analysis_slices.loc[test_data_inds,'research study'].to_list().count('AF_MAYO')]\n",
    "    AFHSS_train_val_test = [df_analysis_slices.loc[train_data_inds,'research study'].to_list().count('AF_HSS'),\n",
    "                            df_analysis_slices.loc[val_data_inds,'research study'].to_list().count('AF_HSS'),\n",
    "                            df_analysis_slices.loc[test_data_inds,'research study'].to_list().count('AF_HSS')]\n",
    "    vanita_train_val_test = [sum(x.startswith('vanitha') for x in df_analysis_slices.loc[train_data_inds,'research study'].to_list()),\n",
    "                           sum(x.startswith('vanitha') for x in df_analysis_slices.loc[val_data_inds,'research study'].to_list()),\n",
    "                           sum(x.startswith('vanitha') for x in df_analysis_slices.loc[test_data_inds,'research study'].to_list())]\n",
    "    p50_ratio = [p50_train_val_test[i]/total_cases_train_val_test[i] for i in range(len(total_cases_train_val_test))]\n",
    "    AFUCSF_ratio = [AFUCSF_train_val_test[i]/total_cases_train_val_test[i] for i in range(len(total_cases_train_val_test))]\n",
    "    AFMAYO_ratio = [AFMAYO_train_val_test[i]/total_cases_train_val_test[i] for i in range(len(total_cases_train_val_test))]\n",
    "    AFHSS_ratio = [AFHSS_train_val_test[i]/total_cases_train_val_test[i] for i in range(len(total_cases_train_val_test))]\n",
    "    vanita_ratio = [vanita_train_val_test[i]/total_cases_train_val_test[i] for i in range(len(total_cases_train_val_test))]\n",
    "\n",
    "    timepoint_1 = [df_analysis_slices.loc[train_data_inds,'timepoint'].to_list().count(1),\n",
    "                           df_analysis_slices.loc[val_data_inds,'timepoint'].to_list().count(1),\n",
    "                           df_analysis_slices.loc[test_data_inds,'timepoint'].to_list().count(1)]\n",
    "    timepoint_2 = [df_analysis_slices.loc[train_data_inds,'timepoint'].to_list().count(2),\n",
    "                           df_analysis_slices.loc[val_data_inds,'timepoint'].to_list().count(2),\n",
    "                           df_analysis_slices.loc[test_data_inds,'timepoint'].to_list().count(2)]\n",
    "    timepoint_3 = [df_analysis_slices.loc[train_data_inds,'timepoint'].to_list().count(3),\n",
    "                           df_analysis_slices.loc[val_data_inds,'timepoint'].to_list().count(3),\n",
    "                           df_analysis_slices.loc[test_data_inds,'timepoint'].to_list().count(3)]\n",
    "    timepoint_4up = [len(df_analysis_slices.loc[train_data_inds])-(timepoint_1[0]+timepoint_2[0]+timepoint_3[0]),\n",
    "                    len(df_analysis_slices.loc[val_data_inds])-(timepoint_1[1]+timepoint_2[1]+timepoint_3[1]),\n",
    "                    len(df_analysis_slices.loc[test_data_inds])-(timepoint_1[2]+timepoint_2[2]+timepoint_3[2])]\n",
    "    train_timepoints = np.round(np.array([timepoint_1[0], timepoint_2[0], timepoint_3[0], timepoint_4up[0]])/len(df_analysis_slices.loc[train_data_inds]),2)\n",
    "    val_timepoints = np.round(np.array([timepoint_1[1], timepoint_2[1], timepoint_3[1], timepoint_4up[1]])/len(df_analysis_slices.loc[val_data_inds]),2)\n",
    "    test_timepoints = np.round(np.array([timepoint_1[2], timepoint_2[2], timepoint_3[2], timepoint_4up[2]])/len(df_analysis_slices.loc[test_data_inds]),2)\n",
    "    timepoints_train_val_test=[str(train_timepoints[0])+':'+str(train_timepoints[1])+':'+str(train_timepoints[2])+':'+str(train_timepoints[3]),\n",
    "                               str(val_timepoints[0])+':'+str(val_timepoints[1])+':'+str(val_timepoints[2])+':'+str(val_timepoints[3]),\n",
    "                              str(test_timepoints[0])+':'+str(test_timepoints[1])+':'+str(test_timepoints[2])+':'+str(test_timepoints[3])]\n",
    "\n",
    "    data = {'set': ['train','val','test'],\n",
    "            'age avg':age_mean_train_val_test, \n",
    "            'age stdev':age_std_train_val_test, \n",
    "            'age median':age_median_train_val_test,\n",
    "            'weight avg':weight_mean_train_val_test, \n",
    "            'weight std':weight_std_train_val_test, \n",
    "            'weight median':weight_median_train_val_test,\n",
    "            '# cases':total_cases_train_val_test,\n",
    "            '# females':sex_F_train_val_test, \n",
    "            '# males':sex_M_train_val_test, \n",
    "            'M:F':sex_ratio_MtoF_train_val_test,\n",
    "            'P50 % in set':p50_ratio,\n",
    "            'AF_study % in set':str([AFUCSF_ratio,AFMAYO_ratio,AFHSS_ratio]),\n",
    "            'vanitha % in set':vanita_ratio,\n",
    "            'timepoint ratio 1:2:3:4+':timepoints_train_val_test\n",
    "            }\n",
    "\n",
    "    #Save your dataframe\n",
    "    df_analysis_slices.to_csv(save_path.replace('.csv','_'+str(k)+'.csv'),index = False)\n",
    "                          \n",
    "    df_metadata = pd.DataFrame(data)\n",
    "    df_metadata_folds = df_metadata_folds.append(df_metadata)\n",
    "\n",
    "\n",
    "df_metadata_folds.to_csv(save_path_metadata,index = False)\n",
    "df_metadata_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfbd2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2397ca0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700760ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29a84b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
